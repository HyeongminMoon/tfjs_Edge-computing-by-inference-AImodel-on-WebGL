<head>
    <style>
      @import url(//fonts.googleapis.com/earlyaccess/jejumyeongjo.css);

      .jm-font{
          font-family: 'Jeju Myeongjo', serif;/*웹 폰트 지정*/
          color: black;
      }
      .b {
        background-color: #BBDEFB;
        padding: 10px 20px;
      }
    </style>
</head>

{% if page.title == "Home" %}

<h5 class="jm-font"> <b>머리말</b> </h5>
<p class="jm-font"> - 이번에는 Tensorflow js와 WebGL을 통해 서버에서가 아닌 기기의 자원으로 AI가 동작하게 하는 방법을 알아본다. 더불어 이 과정에서 다른 확장자의 모델끼리의 변환과
                      경량화에 대해서도 조금 다룬다.</p>
<p class="jm-font"> - 평소에 이런 고민을 자주 하곤 한다. 기술이 비약적으로 발전하면서 개인이 좋은 성능의 컴퓨터를 가질 수 있게되었다. 그것도 주머니속에 넣고 다닐 수 있는 스마트폰은
                      거의 모두가 가지고 있다. 그 외에도 데스크탑, 노트북, 심지어는 자동차 등의 다양한 기기에도 꽤 좋은 성능의 컴퓨터가 들어간다. 근데 이 좋은 성능의 컴퓨터들을 항상
                      100% 사용하지는 않는다. 그래픽 작업이 필요해서 비싸게 구입한 그래픽카드는 웹 서핑하는 동안 차갑게 식어있다. 나는 가끔 이런 자원들이 아깝다고 생각한다. 티끌모아
                      태산이라고 이러한 자원들을 긁어모아 사용할 수 있다면 그또한 얼마나 효율적인 자원활용이 아니겠는가.</p>
<p class="jm-font"> - 자연스럽게 엣지컴퓨팅에 대해서도 관심을 가지게 되었다. 엣지컴퓨팅이란 이러한 사용자 주변의 자원들을 분산하여 활용하는 것을 말한다. 만약 내가 AI를 이용해 작동하는
                      어플리케이션을 배포했다면, 큰 GPU들이 있는 서버를 가지고 있어야하고 모든 사용자들의 AI동작은 이 서버에서 동작한다. 이때 만약 사용자들의 개인pc, 스마트폰에서 AI가
                      알아서 자원을 긁어와서 동작한다면? 서버의 자원부하가 해소된다. 그리고 내 손안에서 자글자글하게 AI가 돌아가고 있다는 것이 꽤나 재밌다!</p>
<p class="jm-font"> - 그리고 WebGL을 통해 AI를 동작해야하는 가장 핵심 이유는 환경이다. 딥러닝을 조금이라도 개발해봤으면 알겠지만 CUDA, CuDNN 덕분에 개발환경 구축이 꽤나 까다롭고,
                      따라서 AI서비스를 구축했을 때 사용자들에게 모두 CUDA를 설치하라고 할 수는 없는 노릇이다. WebGL은 웹에서 화려한 그래픽동작을 위해 사용자의 GPU를 알아서 끌어다 쓰는
                      기능을 가지고 있으므로, 결론적으로 CUDA없이 사용자의 GPU를 사용하여 AI를 동작할 수 있게되는 아주 중요한 의의를 가진다.</p>
<br></br>

<h5 class="jm-font"> <b>요약</b> </h5>
<p class="jm-font"> 1. tfjs환경: 텐서플로우에서 tfjs라는 자바스크립트용 API를 제공한다. html에서 라이브러리를 바로 import하는 방법과, nodejs같은 패키지를 사용하는 방법이 있다.
    그리고 백엔드도 여러가지 선택 할 수 있는데, 자세한 내용은 <a href="https://www.tensorflow.org/js?hl=ko">공식 홈페이지</a>나 <a href="https://github.com/tensorflow/tfjs">깃허브 레파지토리</a>를 참고하는 것이 좋다.</p>
<p class="jm-font"> 2. 모델: 위 tfjs공식 홈페이지에서 튜토리얼 모델들을 많이 제공한다. 이를 사용하거나 직접 커스텀 모델을 사용가능 한 폼으로 바꾸는 방법이 있다. <a href="https://www.tensorflow.org/js/guide/conversion?hl=ko">이곳</a>
    에서 tensorflow saved model, keras모델을 tfjs에서 사용할 수 있도록 변환기를 제공한다. pytorch모델 등의 다른 형식의 모델이 있다면 tensorflow 모델로 변환해야한다. 그리고 tensorflow lite를 사용할 수도 있다. 
    자세한건 뒤에서 다시 다루겠다.</p>
<br></br>

<h5 class="jm-font"> <b>tensorflow javascript</b> </h5>
<p class="jm-font"> 우선 API는 <a herf="https://js.tensorflow.org/api/latest/">여기서</a> 확인할 수 있다. 상단에 보면 여러가지 환경에 따라 API문서가 정리되어 있다.</p>
<p class="jm-font"> 이것 저것 시도해보면서 얻은 팁(사실상 오류.. 지금 tfjs를 사용하기 껄끄러운 이유)들이 있다. tfjs의 default부터 살펴보자면 사용자의 GPU를 끌어다 쓰기위해 WebGL백엔드를 사용한다.
    WebCPU나 WebGPU등도 제공하지만 깃허브 레파지토리의 이슈들을 살펴보면 WebGL이외의 것들은 알파버전이라 오류도 많고 gpu효율도 별로인듯 하다. 근데 이 WebGL에 심각한 오류가 있다. 튜토리얼에서
    제공하는 가벼운 모델(대부분 mobilenet backbone이더라) 사용에는 문제가 없지만, 커스텀 모델 등 큰 모델을 사용하려다가는 메모리 누수가 생긴다. 이슈와 더불어 내 개인적인 생각을 통해 원인을
    파악했는데 아마도 tensorflow는 gpu를 사용할 때에 gpu할당을 실시간으로 하지 않고 사전에 미리 할당하는데 이것이 CUDA가 아닌 WebGL에서 불안정하게 동작하면서 누수가 발생하는 것이 아닌가 싶다.
    이유야 불문하고, 어쨌든 이로인해 큰 모델을 사용하다간 웹자체가 터지고, 운 나쁘면 컴퓨터가 셧다운되는 기이한 현상을 볼 수 있다.</p>
<p class="jm-font"> 결론은 다행히 아직 활발하게 이 문제를 해결하려고 하고 있는 듯 하니 큰 모델은 우선 좀 기다리자. 튜토리얼로 제공되는 가벼운 모델로 학습하여 동작할만한 서비스는 사용해도 좋다.
    또는 tensorflow lite를 사용하면 그나마 낫다. 아직 tensorflow lite javascript는 알파버전이라 cpu에서만 동작한다. (결국 얘도 기다려야한다.) 그래도 tflite에서 제공하는 양자화등의 방법으로 모델을 경량화하면
    WebGL gpu로 돌리는 것보다는 조금 더 무거운 모델을 돌릴 수 있다. 그리고 적어도 느릴지언정 메모리 누수의 문제는 없다.</p>
<br></br>

<h5 class="jm-font"> <b>모델 변환 & 경량화</b> </h5>
<p class="jm-font"> 상기된 이유로 사실 이 글의 핵심은 여기다. pytorch, onnx, tensorflow 간 모델 변환과, tensorflow->tfjs, tensorflow->tflite, 그리고 양자화를 통한 경량화를 할 수 있다.
    이부분은 좀 어렵다. 미래의 내가 봤을때 조금이나마 도움이 되도록 작성한다.</p>
<p class="jm-font"> 우선 pytorch나 tensorflow는 라이브러리로 많이 들어봤지만 onnx는 조금  생소한데, onnx는 쉽게말하면 모델 통합을 위한 라이브러리다. 개발도 할 수있다곤 알고있다. 즉, 모든 모델의 변환은 onnx를 통하면 할 수 있다.
    pytorch->onnx->tensorflow나 pytorch->onnx->caffe 등의 순으로 하면 된다는 것이다. github에 tensorflow2onnx, onnx2pytorch 이런식으로 많은 사람들이 만들어놓은 레파지토리를 참고하여
    변경하면 된다. 이때 주의해야할 부분은 UpSampling에 관한 부분인데, 라이브러리들의 버전이 넘어가면서(특히 tensorflow) UpSampling방식이 바뀌어서 이 부분에서 모델변환에 오류가 날 수 있다.
    따라서 커스텀 모델에 UpSampling layer가 포함되어 있다면 버전확인을 해보자. 나같은 경우는 변환과정에서 onnx버전을 10, 11로 했을때 오류가 났으나 9로하니 오류가 해결되었다. 찾아보니 onnx버전
    converter도 있는데 궁지에 몰렸다면 이것도 참고해보기 바란다.</p>
<p class="jm-font"> 어떻게든 당신의 모델을 tensorflow 형식으로 바꿨다면 축하한다. 여기까지 왔다면 그 뒤는 간단하다. (tfjs든 tflite든 결국 tensorflow에서 만들었기에 당연히 호환이 잘된다.)
    tfjs에서 제공하는 변환기를 사용해서 tfjs 형식으로 바꿔주면 WebGL등의 방식을 이용할 수 있다. api문서를 참고하여 당신의 inference코드를 javascript로 번역해주면 끝이다. tfjs 깃허브를 참고하면
    변환기에서 양자화 하는 인수가 있다. 사용해보니 그다지 체감은 안된다.</p>
<p class="jm-font"> 다음은 tflite를 사용하는 방법이다. tfjs와 똑같이 사용하는 것은 아니고, tflite용 api가 따로 있다. 알파버전으로 제공하는 기능이 매우 한정되어있고, cpu밖에 쓰지 못한다.
    그럼에도 불구하고 이 방식을 추천한다. gpu활용은 아직 문제가 많으니 경량화라도 잘해서 문제없는 cpu로 동작시키는게 맘편하달까. tflite 모델은 이름에서부터 경량화를 목적으로 함이 드러나듯이
     여러가지 양자화 방법을 제공한다. 여기를 참고하여 커스텀 모델에 맞게 양자화 해보길 바란다.</p>
<br></br>

<h5 class="jm-font"> <b>예제</b> </h5>
컴퓨터 사양에 따라 조금 시간이 걸릴 수 있다.
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/deeplab"></script>
<div class="row g-5 mb-5">
    <div class="col-md-6">
        
        <img id="img" src="https://user-images.githubusercontent.com/32811724/147175762-2749386f-3c38-4a8a-89a5-80928e00e381.jpg" alt="" width="90%"/>
        <input type="button" id="button1" onclick="predict();" value="demo"/>
        <form method="post" enctype="multipart/form-data">
        <input type="file" id="button2" onchange="changeimg(this);" accpet="image/*" value="upload image"/>
        </form>
    </div>
    <div class="col-md-6">
        <canvas id="person">
        </canvas>
    </div>
</div>
<script>
function changeimg(input) {
    var file = input.files[0];
    document.getElementById("img").src = URL.createObjectURL(file);
    predict();
}
    
const loadModel = async () => {
  const modelName = 'ade20k';   // set to your preferred model, either `pascal`, `cityscapes` or `ade20k`
  const quantizationBytes = 2;  // either 1, 2 or 4
  return await deeplab.load({base: modelName, quantizationBytes});
};
    
const translateSegmentationMap = async (segmentationMap) => {
  return await deeplab.toSegmentationImage(
      deeplab.getColormap(model), deeplab.getLabels(model), segmentationMap)
};

const img = document.getElementById('img');
// var pixels = tf.browser.fromPixels(img);
// pixels = tf.image.resizeBilinear(pixels, [227, 500]);//227, 500, 3
// console.log(pixels)
// const input = tf.zeros([227, 500, 3]);
// ...
var canvas = document.getElementById('person');
canvas.style.width = '90%';
ctx = canvas.getContext('2d');
const demo_clear_img = new Image();
demo_clear_img.src = "/assets/img/demo_clear.jpg";
demo_clear_img.onload = function(){
    ctx.drawImage(demo_clear_img, 0, 0)
}
    
const predict = async () => {
    
    const model = await loadModel();

    const output = await model.segment(img).then((output) =>{
//         console.timeEnd("predict time");
        const {legend, height, width, segmentationMap} = output
//         console.log(`The predicted classes are ${JSON.stringify(legend)}`);
        //             console.log(segmentationMap);
        var canvas = document.getElementById('person');
            canvas.style.width = '90%';
        canvas.width = width;
        canvas.height = height;
//         console.log(width, height)
        ctx = canvas.getContext('2d');
        var imgdata = new ImageData(segmentationMap, width, height);
        ctx.putImageData(imgdata, 0, 0);
    });

};


// predict();
</script>


<div class="row g-5 mb-5" style="text-align:left">
<hr width = "90%" color = "black">
<h5> Project Links </h5>
      {% for item in site.data.publications.featured %}
        <p><a href="{{ item.url }}">{{ item.name }}</a></p>
      {% endfor %}
    {% else %}
      includes publications.html
      {% for item in site.data.publications.index %}
        <p><a href="{{ item.url }}">{{ item.name }}</a></p>
      {% endfor %}
    {% endif %}
</div>
